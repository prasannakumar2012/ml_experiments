{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interp, stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import *\n",
    "import sklearn.preprocessing as prep\n",
    "\n",
    "import xgboost\n",
    "import tensorflow as tf\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_rows', 100) \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenericAutoencoder(object):\n",
    "    def __init__(self, ae_shape = [2,1,2] , transfer_function_arr=[tf.nn.sigmoid], optimizer = tf.train.AdamOptimizer()):\n",
    "        self.ae_shape = ae_shape\n",
    "        self.transfer_function_arr = transfer_function_arr\n",
    "\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights\n",
    "        self.all_output = []\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.ae_shape[0]])\n",
    "#         self.y = tf.placeholder(tf.float32, [None, self.ae_shape[-1]])\n",
    "        self.y = tf.placeholder(tf.float32, [None])\n",
    "        self.hidden = self.x\n",
    "        self.all_output.append(self.hidden)\n",
    "        for index in range(0,len(self.ae_shape)-2):\n",
    "            self.hidden = self.transfer_function_arr[index](tf.add(tf.matmul(self.hidden, self.weights[\"w\"+str(index+1)]), \n",
    "                                               self.weights[\"b\"+str(index+1)]))\n",
    "            self.all_output.append(self.hidden)\n",
    "        \n",
    "        self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights[\"w\"+str(index+2)]), \n",
    "                                     self.weights[\"b\"+str(index+2)])\n",
    "\n",
    "        self.all_output.append(self.reconstruction)\n",
    "        # cost\n",
    "        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.y), 2.0))\n",
    "        \n",
    "        \n",
    "#         self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits \n",
    "#                                                                     = self.reconstruction, labels = self.y))\n",
    "        \n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "        low = 0\n",
    "        high = 1\n",
    "        for index in range(0,len(self.ae_shape)-1):\n",
    "            all_weights[\"w\"+str(index+1)] = tf.Variable(tf.random_uniform([self.ae_shape[index],\n",
    "                                                                           self.ae_shape[index+1]], minval = low, \n",
    "                                                                          maxval = high, dtype = tf.float32))\n",
    "            all_weights[\"b\"+str(index+1)] = tf.Variable(tf.random_uniform([self.ae_shape[index+1]], minval = low, \n",
    "                                                                          maxval = high, dtype = tf.float32))\n",
    "        return all_weights\n",
    "\n",
    "    def partial_fit(self, X, Y):\n",
    "        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X, self.y: Y})\n",
    "        return cost\n",
    "\n",
    "    def calc_total_cost(self, X):\n",
    "        return self.sess.run(self.cost, feed_dict = {self.x: X})\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.sess.run(self.hidden, feed_dict={self.x: X})\n",
    "\n",
    "    def generate(self, hidden = None):\n",
    "        if hidden is None:\n",
    "            hidden = self.sess.run(tf.random_normal([1, self.n_hidden]))\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.hidden: hidden})\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.x: X})\n",
    "\n",
    "    def getWeights(self):\n",
    "        return self.sess.run(self.weights)\n",
    "\n",
    "    def getBiases(self):\n",
    "        return self.sess.run(self.weights)\n",
    "    \n",
    "    def getAllOutPut(self,X):\n",
    "        return self.sess.run(self.all_output, feed_dict={self.x: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_max_scale(data):\n",
    "    preprocessor = prep.MinMaxScaler().fit(data)\n",
    "    data = preprocessor.transform(data)\n",
    "    return data\n",
    "\n",
    "def standard_scale(data):\n",
    "    preprocessor = prep.StandardScaler().fit(data)\n",
    "    data = preprocessor.transform(data)\n",
    "    return data\n",
    "\n",
    "def get_random_block_from_data(x, y, batch_size):\n",
    "    start_index = np.random.randint(0, len(data) - batch_size)\n",
    "    return (x[start_index:(start_index + batch_size)], y[start_index:(start_index + batch_size)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 110)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"/Users/prasanna/Downloads/\"\n",
    "file_name = \"DS_Tech_Review_Dataset (1).txt\"\n",
    "data = pd.read_csv(folder_path + file_name, sep=\"|\")\n",
    "#data.describe()\n",
    "#data.corr()\n",
    "age_cols = [col for col in data.columns.values if \"AGE\" in col]\n",
    "relevant_age_cols = [col for col in data.columns.values if \"AGE\" in col and (\"_\" in col or \"UP\" in col)]\n",
    "data.dropna(thresh=len(data)*0.1, axis=1, inplace=True)\n",
    "\n",
    "same_value_cols =[]\n",
    "for col in data.columns.values:\n",
    "    unique_cols = data[col].unique()\n",
    "    if len(unique_cols) < 30 :\n",
    "        #print (col, unique_cols)\n",
    "        pass\n",
    "    if len(unique_cols) == 1 or (len(unique_cols) == 2 and np.isnan(unique_cols).any()) :\n",
    "        same_value_cols.append(col)\n",
    "\n",
    "data[\"MAJOR_CREDIT_CARD_LIF\"] = np.where(data[\"MAJOR_CREDIT_CARD_LIF\"].isnull(),\"NA\", data[\"MAJOR_CREDIT_CARD_LIF\"])\n",
    "data.fillna(0,inplace=True)\n",
    "data = pd.get_dummies(data, columns=[\"product\", \"MAJOR_CREDIT_CARD_LIF\"])\n",
    "data.drop(same_value_cols, axis=1, inplace=True)\n",
    "\n",
    "target_values = data[\"target\"].astype('float32').copy()\n",
    "data.drop(\"target\", axis=1, inplace=True)\n",
    "class_names = [\"No\",\"Yes\"]\n",
    "features_names = data.columns.values.tolist()\n",
    "features_data = data.astype('float32').as_matrix()\n",
    "features_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_epochs = 10\n",
    "batch_size = 128*20\n",
    "display_step = 5\n",
    "X_train = min_max_scale(features_data)\n",
    "target_values_re = target_values.reshape(300000,1)\n",
    "n_samples = len(X_train)\n",
    "autoencoder = GenericAutoencoder(\n",
    "    ae_shape=[110, 40, 15, 40, 1],\n",
    "    transfer_function_arr=[tf.nn.sigmoid,tf.nn.sigmoid,tf.nn.sigmoid],\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001))\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "#     total_batch = int(n_samples / batch_size)\n",
    "#     for i in range(total_batch):\n",
    "#         batch_xs = get_random_block_from_data(X_train, target_values_re, batch_size)\n",
    "#         cost = autoencoder.partial_fit(batch_xs[0],batch_xs[1])\n",
    "    cost = autoencoder.partial_fit(X_train, target_values)\n",
    "    avg_cost += cost / n_samples #* batch_size\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", '%d,' % (epoch + 1),\n",
    "              \"Cost:\", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
